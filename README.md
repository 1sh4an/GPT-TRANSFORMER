# LLM using Transformer Architecture #
This project is inspired by the paper #"Attention is All You Need"# and guided by resources from Andrej Karpathy for creating a large language model (LLM) from scratch using transformers. 

## Please Note : ##
- Due to software contraints the model has been trained on a limited text corpus reusulting in low accuracy.
- The accuracy can be significantly improved by increasing the data provided, though would need significantly better software to train.

  wizard_of_oz.txt has been used to train the model. The generated text has been added to the repository.

